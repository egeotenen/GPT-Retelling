{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86832fba",
   "metadata": {},
   "source": [
    "### 0. Upload necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9797b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#!pip3 install nltk\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1bce4f",
   "metadata": {},
   "source": [
    "### 0.1. Read Dataset ( N = 114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8583f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('all_gpt_human_only_text.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3fbb9",
   "metadata": {},
   "source": [
    "### 0.1.1 Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200d4329",
   "metadata": {},
   "source": [
    "Get rid of extra white spaces in the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fff8eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip(dataframe):\n",
    "    return [s.strip() for s in dataframe]\n",
    "\n",
    "df= df.apply(strip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f9a68",
   "metadata": {},
   "source": [
    "## 1. Pos Tagging with Less Category Separate for Each Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391d4ba0",
   "metadata": {},
   "source": [
    "Each story's pos taggings completed separately, and counted for the story itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70287a7c",
   "metadata": {},
   "source": [
    "### 1.1. Preprocess and Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d3cbed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data from stopwords, tokenize it, and tag\n",
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def create_tokenized(text):\n",
    "    tokenized = word_tokenize(text)\n",
    "    'tag the tokens'\n",
    "    tagged = nltk.pos_tag(tokenized)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c49e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter and get words as tokens into a new column \n",
    "for column in df.columns:\n",
    "    df[f'tokenized_{column}'] = df[column].apply(lambda x: create_tokenized(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130eba50",
   "metadata": {},
   "source": [
    "### 1.2. Count the pos-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "166c6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the tags\n",
    "def count_tags(mydata):\n",
    "    counter_list=[]\n",
    "    for i in mydata:\n",
    "        counter_list.append(i[1])\n",
    "\n",
    "    tag_counts = Counter(counter_list) \n",
    "    return tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6412d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [col for col in df.columns if col.startswith('tokenized_')]:\n",
    "    df[f'{column}_tag_counts'] = df[column].apply(lambda x: count_tags(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b4960",
   "metadata": {},
   "source": [
    "### 1.3. Merge relevant categories together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b642c70c",
   "metadata": {},
   "source": [
    "Count tags for all nouns (plural nouns, proper nouns etc.), verbs, adjectives, adverbs, pronouns and prepositions/conjuctions/determiners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69483d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [col for col in df.columns if col.endswith('tag_counts')]:\n",
    "    df[f'{column[:-7]}_noun'] = [*(df[column].apply(lambda x: (x['NN'] + x['NNS'] + x['NNP']+ x['NNPS'])))]\n",
    "    df[f'{column[:-7]}_verb'] = [*(df[column].apply(lambda x: (x['VB'] + x['VBN'] + x['VBG']+ x['VBZ'] + x['VBP']+x['VBD'])))]\n",
    "    df[f'{column[:-7]}_adj'] = [*(df[column].apply(lambda x: (x['JJ'] + x['JJS'] + x['JJR'])))]\n",
    "    df[f'{column[:-7]}_adv'] = [*(df[column].apply(lambda x: (x['RB'] + x['RBR'] + x['WRB'] +x['RBS'])))]\n",
    "    df[f'{column[:-7]}_pron'] = [*(df[column].apply(lambda x: (x['PRP'] + x['PRP$'] + x['WB']+ x['WB$'])))]\n",
    "    df[f'{column[:-7]}_con_det_prep'] = [*(df[column].apply(lambda x: (x['DT'] + x['IN'] + x['UH']+ x['TO']+ x['WDT']+ x['EX'])))]\n",
    "    df[f'{column[:-7]}_prep'] = [*(df[column].apply(lambda x: (x['IN'])))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e5677d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it to the folder\n",
    "df.to_excel('df_counts.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d493e476",
   "metadata": {},
   "source": [
    "### 2. Count Negations Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56819de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('all_gpt_human_only_text.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e630ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_negation_only(text):\n",
    "    tokenized_dict = {}\n",
    "    tokenized = word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "    to_remove = set()\n",
    "    for i in range(len(tagged)):\n",
    "        if tagged[i][1] in {'RB', 'RBS', 'WRB', 'RBR'} and tagged[i][0].lower() in {'not', \"n't\"}:\n",
    "            pass\n",
    "        elif tagged[i][1] in {'RB', 'RBS', 'WRB','RBR'} and tagged[i][0].lower() not in {'not', \"n't\"}:\n",
    "            to_remove.add(i)\n",
    "\n",
    "        new_tagged = [tagged[i] for i in range(len(tagged)) if i not in to_remove]        \n",
    "\n",
    "    return new_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65235623",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter and get words as tokens\n",
    "for column in df.columns:\n",
    "    df[f'tokenized_{column}'] = df[column].apply(lambda x: tokenize_with_negation_only(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7064f4",
   "metadata": {},
   "source": [
    "### 2.2. Count the pos-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f35bf2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the tags\n",
    "def count_tags(mydata):\n",
    "    counter_list=[]\n",
    "    for i in mydata:\n",
    "        counter_list.append(i[1])\n",
    "\n",
    "    tag_counts = Counter(counter_list) \n",
    "    return tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef109eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [col for col in df.columns if col.startswith('tokenized_')]:\n",
    "    df[f'{column}_tag_counts'] = df[column].apply(lambda x: count_tags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a575d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      {'DT': 16, 'NN': 23, 'WP': 3, 'VBD': 11, 'IN':...\n",
       "1      {'NNP': 4, 'VBD': 14, 'VBG': 7, 'JJ': 9, 'CD':...\n",
       "2      {'DT': 20, 'NNP': 3, 'VBD': 17, 'TO': 7, 'VB':...\n",
       "3      {'DT': 14, 'IN': 14, 'PRP$': 2, 'NN': 22, ',':...\n",
       "4      {'VBP': 7, 'PRP': 26, 'VBD': 18, 'DT': 11, 'NN...\n",
       "                             ...                        \n",
       "111    {'NNP': 6, 'VBZ': 4, 'PRP$': 6, 'NN': 29, '``'...\n",
       "112    {'EX': 1, 'VBD': 19, 'DT': 31, 'NN': 36, 'WP':...\n",
       "113    {'EX': 1, 'VBD': 18, 'DT': 20, 'NN': 32, 'WP':...\n",
       "114    {'CD': 1, 'NN': 30, 'DT': 18, 'VBD': 12, 'VBG'...\n",
       "115    {'DT': 15, 'NN': 24, 'VBD': 12, 'IN': 16, 'JJ'...\n",
       "Name: tokenized_STORY_tag_counts, Length: 116, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tokenized_STORY_tag_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0511a1f1",
   "metadata": {},
   "source": [
    "### 2.3. Merge relevant categories together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd50918",
   "metadata": {},
   "source": [
    "Count tags for all nouns (plural nouns, proper nouns etc.), verbs, adjectives, adverbs, pronouns and prepositions/conjuctions/determiners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "810be6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [col for col in df.columns if col.endswith('tag_counts')]:\n",
    "    df[f'{column[:-7]}_noun'] = [*(df[column].apply(lambda x: (x['NN'] + x['NNS'] + x['NNP']+ x['NNPS'])))]\n",
    "    df[f'{column[:-7]}_verb'] = [*(df[column].apply(lambda x: (x['VB'] + x['VBN'] + x['VBG']+ x['VBZ'] + x['VBP']+x['VBD'])))]\n",
    "    df[f'{column[:-7]}_adj'] = [*(df[column].apply(lambda x: (x['JJ'] + x['JJS'] + x['JJR'])))]\n",
    "    df[f'{column[:-7]}_adv'] = [*(df[column].apply(lambda x: (x['RB'] + x['RBR'] + x['WRB'] + + x['RBS'])))]\n",
    "    df[f'{column[:-7]}_pron'] = [*(df[column].apply(lambda x: (x['PRP'] + x['PRP$'] + x['WB']+ x['WB$'])))]\n",
    "    df[f'{column[:-7]}_con_det_prep'] = [*(df[column].apply(lambda x: (x['DT'] + x['IN'] + x['UH']+ x['TO']+ x['WDT']+ x['EX'])))]\n",
    "    df[f'{column[:-7]}_prep'] = [*(df[column].apply(lambda x: (x['IN'])))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7c2713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('df_negations.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "975ba4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      A school teacher who taught for high school Ki...\n",
       "1      Abigail was feeling sad one day so she started...\n",
       "2      All Jennifer wanted was to go to the punk rock...\n",
       "3      All of his life, Jeff was something of an unde...\n",
       "4      Have you ever had a child?  Do you plan to?  I...\n",
       "                             ...                        \n",
       "111    Michael asks his dad \"Dad, how much money do y...\n",
       "112    Once there was a man who worked for the fire d...\n",
       "113    There was a boy who always wanted a pet goldfi...\n",
       "114    One day a girl was walking her dog when a squi...\n",
       "115    A troll lived alone in a very large cave deep ...\n",
       "Name: STORY, Length: 116, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['STORY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d95fbc",
   "metadata": {},
   "source": [
    "## Now, you have both negation counts (df_negations.xlsx), and part-of-speech tagging (grammatical categories) counts (df_counts.xlsx). \n",
    "\n",
    "You can manually gather storyID, and emotion categories of the stories from the master file(OSF Project page --> Affect Preservation--> Master.Affect2.Generate.Stories.Study.ChatGPT.Final.csv) and change the format of data from short to long for conducting analyses at R. \n",
    "\n",
    "You can also use the final file we added storyID, emotion variables and modify it to long format, it is named as 'df_counts_emotions.xlsx' in OSF Project page --> Word Count and Part of Speechs --> 'partOfSpeech_StoryIDs_Emotions.xlsx'\n",
    "\n",
    "Also please see the below information for the explanation of the tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5260a15f",
   "metadata": {},
   "source": [
    "### Tags:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c41a6e",
   "metadata": {},
   "source": [
    "CC: It is the conjunction of coordinating\n",
    "\n",
    "CD: It is a digit of cardinal\n",
    "\n",
    "DT: It is the determiner\n",
    "\n",
    "EX: Existential\n",
    "\n",
    "FW: It is a foreign word\n",
    "\n",
    "IN: Preposition and conjunction\n",
    "\n",
    "JJ: Adjective\n",
    "\n",
    "JJR and JJS: Adjective and superlative\n",
    "\n",
    "LS: List marker\n",
    "\n",
    "MD: Modal\n",
    "\n",
    "NN: Singular noun\n",
    "\n",
    "NNS, NNP, NNPS: Proper and plural noun\n",
    "\n",
    "PDT: Predeterminer\n",
    "\n",
    "WRB: Adverb of wh\n",
    "\n",
    "WP$: Possessive wh\n",
    "\n",
    "WP: Pronoun of wh\n",
    "\n",
    "WDT: Determiner of wp\n",
    "VBZ: Verb\n",
    "\n",
    "VBP, VBN, VBG, VBD, VB: Forms of verbs\n",
    "\n",
    "UH: Interjection\n",
    "\n",
    "\n",
    "TO: To go\n",
    "\n",
    "RP: Particle\n",
    "\n",
    "RBS, RB, RBR: Adverb\n",
    "\n",
    "PRP, PRP$: Pronoun personal and professional"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
