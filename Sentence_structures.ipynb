{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edb7c72f",
   "metadata": {},
   "source": [
    "### 0. Initizalize Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30df98ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scipy,seaborn\n",
    "import seaborn as sns\n",
    "from scipy.stats import kstest\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#!pip install -U plotly\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "#!pip3 install nltk\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "import math\n",
    "from scipy.stats import shapiro \n",
    "from scipy.stats import lognorm\n",
    "from scipy.stats import mannwhitneyu\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#!pip3 install openpyxl\n",
    "import openpyxl as px\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1bce4f",
   "metadata": {},
   "source": [
    "### 0.1. Read Dataset ( N = 114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "8583f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('all_gpt_human_only_text.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3fbb9",
   "metadata": {},
   "source": [
    "### 0.1.1 Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200d4329",
   "metadata": {},
   "source": [
    "Get rid of extra white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "3fff8eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip(dataframe):\n",
    "    return [s.strip() for s in dataframe]\n",
    "\n",
    "df= df.apply(strip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391d4ba0",
   "metadata": {},
   "source": [
    "Each story's pos taggings completed separately, and counted for the story itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70287a7c",
   "metadata": {},
   "source": [
    "### 1.1. Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "0de9cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_list = '''(),-[]{};*:'\"\\,<>/@_~ '''\n",
    "punc_list = [*punc_list]\n",
    "\n",
    "def map_item_to_category(item):\n",
    "    if item in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        return 'NN'\n",
    "    elif item in ['JJ', 'JJS', 'JJR']:\n",
    "        return 'JJ'\n",
    "    elif item in ['RB', 'RBR', 'WRB', 'RBS']:\n",
    "        return 'RB'\n",
    "    elif item in ['PRP', 'PRP$', 'WP', 'WP$']:\n",
    "        return 'PRP'\n",
    "    elif item in punc_list:\n",
    "        return ''\n",
    "    else:\n",
    "        return item\n",
    "\n",
    "def sentence_splitter(mylist):   #separately take sentences which seperated with '.'\n",
    "    # Create an empty list to store the separate lists\n",
    "    result = []\n",
    "    # Create a temporary list to store the chunks\n",
    "    temp = []\n",
    "\n",
    "    # Loop through each element in the list\n",
    "    for item in mylist:\n",
    "        if item not in [*punc_list]: \n",
    "            # Check if the element is a period\n",
    "            if item == ('.' or '?' or '!'):\n",
    "                # If it is, add the temporary list to the result list\n",
    "                result.append(temp)\n",
    "                # Reset the temporary list\n",
    "                temp = []\n",
    "            else:\n",
    "                # If it's not a period, append the element to the temporary list\n",
    "                temp.append(item)\n",
    "\n",
    "    # Add the last temporary list to the result list, in case the list ends without a period\n",
    "    if temp:\n",
    "        result.append(temp)\n",
    "    return(result)\n",
    "\n",
    "def sentence_dna(taglist):\n",
    "    result = []\n",
    "    for sublist in taglist:\n",
    "        # Use join() to concatenate the elements in each sublist into a single string\n",
    "            string = ''.join(sublist)\n",
    "            result.append(string)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "6d3cbed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_sentence_dnas(text):\n",
    "    tags = []\n",
    "    tokenized = word_tokenize(text)\n",
    "    'tag the tokens'\n",
    "    tagged = nltk.pos_tag(tokenized)\n",
    "    for i in tagged:\n",
    "        tags.append(i[1])\n",
    "    for i in range(len(tags)):\n",
    "        tags[i] = map_item_to_category(tags[i]) # makes tags generalized e.g., NN, NNS, NNP become NN \n",
    "    \n",
    "    splitted_sentences = sentence_splitter(tags)        \n",
    "    dna = get_sentence_dna(splitted_sentences)\n",
    "    return dna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "7c49e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the sentence splitter function to each row in some columns\n",
    "for column in df.columns:\n",
    "    df[f'{column}_dna'] = df[column].apply(lambda x: get_split_sentence_dnas(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130eba50",
   "metadata": {},
   "source": [
    "### 1.2. Get Most Frequent Sentence Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "900b17a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def most_freq_sentence_dna(df,column):\n",
    "    # Create an empty dictionary to store word counts\n",
    "    word_counts = {}\n",
    "\n",
    "    # Iterate through each row in the specified column\n",
    "    for index, row in df.iterrows():\n",
    "        # Get the value in the current row of the specified column\n",
    "        text = row[column]\n",
    "\n",
    "        # Split the text into words using space as a delimiter\n",
    "        words = [i for i in text]\n",
    "\n",
    "        # Iterate through each word in the list\n",
    "        for word in words:\n",
    "            # Skip empty strings\n",
    "            if word == '':\n",
    "                continue\n",
    "\n",
    "            # If the word is not in the dictionary, add it with a count of 1\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = 1\n",
    "            # If the word is already in the dictionary, increment its count by 1\n",
    "            else:\n",
    "                word_counts[word] += 1\n",
    "\n",
    "    # Convert the dictionary to a list of tuples (word, count)\n",
    "    result = list(word_counts.items())\n",
    "\n",
    "    # Sort the list based on word counts in descending order\n",
    "    result.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the list of word counts\n",
    "    return result[0:3] # print most freq first three\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "14d6a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['STORY_dna', 'GPT_R1_dna', 'GPT_R2_dna', 'GPT_R3_dna',\n",
    "       'Retell_1_dna', 'Retell_2_dna', 'Retell_3_dna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "e6412d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORY_dna [(\"''PRPVBD\", 7), ('NN', 5), ('PRPVBDDTJJNN', 4)]\n",
      "GPT_R1_dna [('DTJJNNNNVBDVBGNNCCVBDDTNNNNINCDINPRPJJNNNNCCNN', 1), ('PRPVBDJJTOVBPRPINDTJJNNCCVBDDTNN', 1), ('RBPRPVBDDTNNINPRPVBGPRPTOPRPNNNN', 1)]\n",
      "GPT_R2_dna [('DTJJNNRBVBDDTNNNNINNNINCDJJNNNNCCNN', 1), ('PRPVBDVBNCCJJTOVBPRPINDTJJNNCCVBDDTNN', 1), ('PRPRBVBDPRPCCVBDPRPTOPRPNNNNWDTPRPVBDCCVBDNNINPRP', 1)]\n",
      "GPT_R3_dna [('DTJJNNRBVBDDTNNNNINNNINCDJJNNNNCCNN', 1), ('PRPVBDPRPTOPRPNNNNWDTPRPVBDCCVBDNNINPRP', 1), ('DTNNVBDRPNNNNCCNNINDTNN', 1)]\n",
      "Retell_1_dna [('PRPVBDDTJJNN', 3), ('PRPVBDNNNN', 2), (\"''\", 2)]\n",
      "Retell_2_dna [('NNVBDJJ', 2), ('NN', 2), ('VB', 2)]\n",
      "Retell_3_dna [('NN', 3), ('PRPVBDDTJJNN', 2), ('DTJJNNNNVBDVBGINNNCDNNRBPRPVBDCDNNNNINCDJJNN', 1)]\n"
     ]
    }
   ],
   "source": [
    "for col in cols:\n",
    "    print(col, most_freq_sentence_dna(df,col))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
